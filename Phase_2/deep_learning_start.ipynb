{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1cmZnQOm517",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## P&O ISSP: Brain-computer interface voor sturing van een directionele akoestische zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15TgxRRrm52A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, we will start building a basic deep learning implementation for classifying which two of the Stimuli was attended to, when given EEG and both Stimuli as input. \n",
    "\n",
    "\n",
    "One of the ways to process the EEG data is to find specific patterns in the signal. Based on the presence or absence of these patterns we will decide where is the auditory attention. But handcrafting these pattern might be difficult, so we will use convolutional neural network to learn filters which can detect those patterns.\n",
    "\n",
    "The implementation will be in mulitple phases. First, we will get more familiar with keras and the deep learning framework by mimicking the linear regression-based network, but then in a non-linear context. Once we have implemented this, we can start playing with the deep learning architectures and add some blocks, see what different training schemes do, etc...\n",
    "\n",
    "Once we have a working model, we can start to play with the data and see if we can improve the performance. The basic model will transform the EEG to a space where it has to resemble the envelope, and then we will compare performance by calcualtion the correlation between this represenation and both envelopes. instead of only transforming the EEG, we can try to transform the envelopes as well. In this way, both EEG and envelopes get transformed to a common space, and we can compare how similar they are in this latent space. This gives the model more degrees of freedom, to find a representation that is good for both the EEG and the envelopes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJUj1LWm52C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Note**: If keras is  not already installed, execute: !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_xoiZWUm52E",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Activation\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRUN1nnem52N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "* The EEG data preprocessing has been explained in another tutorial.\n",
    "* we have already implemeted method a) linear decoder baseline\n",
    "\n",
    "**Convolutional baseline network**\n",
    "* The first step in the model is a convolutional layer, A (64 x 16) spatio-temporal filter is shifted over the input matrix, containing the EEG.\n",
    "* A rectifying linear unit (ReLu) activation function is used after the convolution step. the kernel size of 16 is chosen because, as is the case in the linear model, we want to look to future EEG to predict the current envelope. the EEG is sampled at fs=64Hz, giving us a temporal resolution of 16/64 = 250ms.\n",
    "* The output of the convolutional block is a (time-window, 1) signal. \n",
    "* In the next step, we calculate the cosine similarity between this signal and both of the envelopes. We will calculate this cosine similarity by applying a *dot product* between the signal and both envelopes. \n",
    "* As a last step, we then have to choose which one of the two attended envelopes is the one we want to choose. We do this by applying a single neuron ( **dense layer** in keras, with a sigmoid activation function. \n",
    "\n",
    "**deep learning model**\n",
    "* the idea here is the same. We still give EEG and envelopes to the model, there are just more processing steps in between before we have to make a decision. \n",
    "* we first apply a one-dimensional convolution to the EEG, with 8 output filters. We can interpret this as kind of a non-linear dimensionality reduction, as the resulting EEG has shape (time-window, 8) instead of the original (time-window, 64) \n",
    "* next, there are some convolutional blocks. These convolutions are applied to both EEG and envelopes. We will have separate track for the EEG ( eg 1 convolutional block) and one for the envelope (eg. also 1 convolutional block). To keep the model stable and simple, we will have one 'track' for the envelopes. Both attended and unattended envelope will be transformed by the same convolutional block, ensuring that the model has to learn to distinguish between the attended and unattended envelope.\n",
    "* after that, we once again compute the dot product and subsequently put the result of this in a sigmoid neuron to reach an end decision.\n",
    "* the possibilities are endless, and we can try to add more convolutional blocks, or even add a recurrent layer ( LSTM blocks) to the model. What is important is that you start from a simple model, and then gradually expand it. this way, if something does not work, it is easier to find the problem, or to revert back to a simpler model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hmbrPpkm52O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "eeg = tf.keras.layers.Input(shape=[time_window, 64])\n",
    "env1 = tf.keras.layers.Input(shape=[time_window, 1])\n",
    "env2 = tf.keras.layers.Input(shape=[time_window, 1])\n",
    "\n",
    "#add model layers\n",
    "## ---- add your code ----here\n",
    "\n",
    "# Classification\n",
    "out1 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(\n",
    "    tf.keras.layers.Flatten()(tf.keras.layers.Concatenate()([cos1, cos2])))\n",
    "\n",
    "# 1 output per batch\n",
    "out = tf.keras.layers.Reshape([1], name=output_name)(out1)\n",
    "model = tf.keras.Model(inputs=[eeg, env1, env2], outputs=[out])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvReB1npm52U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To check the model summary:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we start training the model, we need to make sure that the data is equally balanced. We have attended and unattended envelopes that we give to the model. If we always put the attended envelope at stream 1 and the unattended at stream 2, the model will quickly figure out that it should just always output stream 1 and hence not learn anything. \n",
    "\n",
    "The solution to this is to present each segment of EEG twice, where we swap the envelopes, ( and thus, the labels), from place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def batch_equalizer(eeg, env_1, env_2, labels):\n",
    "    # present each of the eeg segments twice, where the envelopes, and thus the labels \n",
    "    # are swapped around. EEG presented in small segments [bs, window_length, 64]\n",
    "    return (np.concatenate([eeg,eeg], axis=0), np.concatenate([env_1, env_2], axis=0),np.concatenate([ env_2, env_1], axis=0)), np.concatenate([labels, (labels+1)%2], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3mzuy-jm52i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data loading\n",
    "\n",
    "* as you can see, the total amount of data is quite a few GB. this will most probably not fit in your RAM, so we will have to load the data in batches.\n",
    "* Python generators are a great way to do this.\n",
    "* Now we prepare our data to train the model.\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(files)\n",
    "\n",
    "# create tf dataset from generator\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        data_generator)\n",
    "\n",
    "# now you have a dataset, you can perform operations on the fly (  using built-in functions such as 'map', 'window', 'batch', etc)\n",
    "# eg. window the dataset into slices of (EEG, envelope1, envelope2) of a certain length, with a hop size between consecutive slices\n",
    "# batch the data into batches of a certain size\n",
    "# shuffle the data\n",
    "# create a corect label for each sample ( is envelope 1 attended or envelope2 , eg, label 1 or 0 )\n",
    "\n",
    "# using keras, we can easily create a model that can be trained on this dataset, giving this dataset to the model.fit() function\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"Generate data for the Match/Mismatch task.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        files\n",
    "    ):\n",
    "        \"\"\"Initialize the DataGenerator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        files: Sequence[Union[str, pathlib.Path]]\n",
    "            Files to load.\n",
    "        \"\"\"\n",
    "        self.files = files\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, recording_index):\n",
    "        \"\"\"Get data for a certain recording.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        recording_index: int\n",
    "            Index of the recording in this dataset to load data for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[Tuple[tf.Tensor,...], Tuple[np.ndarray,...]]\n",
    "            The features corresponding to the recording_index recording\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the data\n",
    "        # prepare the data for the model   ( eeg, env1, env2)\n",
    "        # return the data\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Load data for the next recording.\n",
    "\n",
    "        Yields\n",
    "        -------\n",
    "        Union[Tuple[tf.Tensor,...], Tuple[np.ndarray,...]]\n",
    "            The features corresponding to the recording_index recording\n",
    "        \"\"\"\n",
    "        for idx in range(self.__len__()):\n",
    "            yield self.__getitem__(idx)\n",
    "\n",
    "            if idx == self.__len__() - 1:\n",
    "                self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Change state at the end of an epoch.\"\"\"\n",
    "\n",
    "        # choose if you want to do something with the data at the end of an epoch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_S_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}