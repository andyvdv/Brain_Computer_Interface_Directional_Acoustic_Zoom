{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1cmZnQOm517",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## P&O ISSP: Brain-computer interface voor sturing van een directionele akoestische zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15TgxRRrm52A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, we will start building a basic deep learning implementation for classifying which two of the Stimuli was attended to, when given EEG and both Stimuli as input. \n",
    "\n",
    "\n",
    "One of the ways to process the EEG data is to find specific patterns in the signal. Based on the presence or absence of these patterns we will decide where is the auditory attention. But handcrafting these pattern might be difficult, so we will use convolutional neural network to learn filters which can detect those patterns.\n",
    "\n",
    "The implementation will be in mulitple phases. First, we will get more familiar with keras and the deep learning framework by mimicking the linear regression-based network, but then in a non-linear context. Once we have implemented this, we can start playing with the deep learning architectures and add some blocks, see what different training schemes do, etc...\n",
    "\n",
    "Once we have a working model, we can start to play with the data and see if we can improve the performance. The basic model will transform the EEG to a space where it has to resemble the envelope, and then we will compare performance by calcualtion the correlation between this represenation and both envelopes. instead of only transforming the EEG, we can try to transform the envelopes as well. In this way, both EEG and envelopes get transformed to a common space, and we can compare how similar they are in this latent space. This gives the model more degrees of freedom, to find a representation that is good for both the EEG and the envelopes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAJUj1LWm52C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Note**: If keras is  not already installed, execute: !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_xoiZWUm52E",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Activation, Lambda, GlobalAveragePooling1D\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Activation, Dot, Flatten, Reshape, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import l2_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Sequence, Union\n",
    "import pathlib\n",
    "import mne\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"Generate data for the Match/Mismatch task.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        files: Sequence[Union[str, pathlib.Path]],\n",
    "        base_path: str = os.getcwd() + \"/data/train_cnn\",\n",
    "        eeg_base_path: str = os.getcwd() + \"/data/eeg\",\n",
    "        time_window: int = 60 * 10\n",
    "    ):\n",
    "        \"\"\"Initialize the DataGenerator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        files: Sequence[Union[str, pathlib.Path]]\n",
    "            Files to load.\n",
    "        \"\"\"\n",
    "        self.files = files\n",
    "        self.base_path = base_path\n",
    "        self.eeg_base_path = eeg_base_path\n",
    "        self.time_window = time_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, recording_index):\n",
    "        eeg_filename = self.files[recording_index]\n",
    "        eeg_data = np.load(os.path.join(self.eeg_base_path, eeg_filename))\n",
    "\n",
    "        attended_filename = str(eeg_data[\"stimulus_attended\"]).replace(\".wav\", \"\") + \".npy\"\n",
    "        unattended_filename = str(eeg_data[\"stimulus_unattended\"]).replace(\".wav\", \"\") + \".npy\"\n",
    "        \n",
    "#         eeg_preprocessed = np.load(os.path.join(self.base_path, eeg_filename.replace(\".npz\", \".npy\")))\n",
    "#         attended_preprocessed = np.load(os.path.join(self.base_path, attended_filename))\n",
    "#         unattended_preprocessed = np.load(os.path.join(self.base_path, unattended_filename))\n",
    "        \n",
    "        eeg_filepath = os.path.join(self.base_path, eeg_filename.replace(\".npz\", \".npy\"))\n",
    "        attended_filepath = os.path.join(self.base_path, attended_filename)\n",
    "        unattended_filepath = os.path.join(self.base_path, unattended_filename)\n",
    "\n",
    "        if not os.path.exists(eeg_filepath) or not os.path.exists(attended_filepath) or not os.path.exists(unattended_filepath):\n",
    "            return None, None, None\n",
    "\n",
    "        eeg_preprocessed = np.load(eeg_filepath)\n",
    "        attended_preprocessed = np.load(attended_filepath)\n",
    "        unattended_preprocessed = np.load(unattended_filepath)\n",
    "    \n",
    "        min_data_len = min(eeg_preprocessed.shape[0], attended_preprocessed.shape[0], unattended_preprocessed.shape[0])\n",
    "        new_data_length = (min_data_len // self.time_window) * self.time_window\n",
    "        \n",
    "        eeg_preprocessed = eeg_preprocessed[:new_data_length]\n",
    "        attended_preprocessed = attended_preprocessed[:new_data_length]\n",
    "        unattended_preprocessed = unattended_preprocessed[:new_data_length]\n",
    "\n",
    "        # Reshape the attended and unattended envelopes to have an extra dimension\n",
    "        attended_preprocessed = attended_preprocessed.reshape(-1, self.time_window, 1)\n",
    "        unattended_preprocessed = unattended_preprocessed.reshape(-1, self.time_window, 1)\n",
    "\n",
    "        # Reshape the EEG data\n",
    "        eeg_preprocessed = eeg_preprocessed.reshape(-1, self.time_window, 64)\n",
    "        \n",
    "        split_index = int(eeg_preprocessed.shape[0] * 0.8)\n",
    "        \n",
    "        eeg_train, eeg_val = np.split(eeg_preprocessed, [split_index], axis=0)\n",
    "        attended_train, attended_val = np.split(attended_preprocessed, [split_index], axis=0)\n",
    "        unattended_train, unattended_val = np.split(unattended_preprocessed, [split_index], axis=0)\n",
    "#         print(\"================\")\n",
    "#         print(eeg_train.shape)\n",
    "#         print(eeg_val.shape)\n",
    "#         print(attended_train.shape)\n",
    "#         print(attended_val.shape)\n",
    "#         print(unattended_train.shape)\n",
    "#         print(unattended_val.shape)\n",
    "#         print(\"================\")\n",
    "        return eeg_train, attended_train, unattended_train, eeg_val, attended_val, unattended_val\n",
    "\n",
    "    def __call__(self):\n",
    "        for idx in range(self.__len__()):\n",
    "            yield self.__getitem__(idx)\n",
    "\n",
    "            if idx == self.__len__() - 1:\n",
    "                self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.files)  \n",
    "        \n",
    "\n",
    "def batch_equalizer(eeg, env_1, env_2, labels):\n",
    "    # present each of the eeg segments twice, where the envelopes, and thus the labels \n",
    "    # are swapped around. EEG presented in small segments [bs, window_length, 64]\n",
    "    return np.concatenate([eeg, eeg], axis=0), np.concatenate([env_1, env_2], axis=0), np.concatenate([ env_2, env_1], axis=0), np.concatenate([labels, (labels+1)%2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.getcwd() + '/data/eeg'\n",
    "all_files = [f for f in os.listdir(data_folder) if f.endswith('.npz')]\n",
    "\n",
    "data_generator = DataGenerator(all_files)\n",
    "# Shuffle the files before splitting\n",
    "# np.random.shuffle(all_files)\n",
    "\n",
    "# # Calculate the split index for an 80-20 train-validation split\n",
    "# split_idx = int(0.8 * len(all_files))\n",
    "\n",
    "# # Split the files into train_files and val_files\n",
    "# train_files = all_files[:split_idx]\n",
    "# val_files = all_files[split_idx:]\n",
    "\n",
    "# print(f\"Total files: {len(all_files)}\")\n",
    "# print(f\"Training files: {len(train_files)}\")\n",
    "# print(f\"Test files: {len(val_files)}\")\n",
    "\n",
    "# train_data_generator = DataGenerator(train_files)\n",
    "# val_data_generator = DataGenerator(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 15:57:05.342728: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# BUILDING THE MODEL\n",
    "time_window = data_generator.time_window\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "eeg = Input(shape=(time_window, 64))\n",
    "env1 = Input(shape=(time_window, 1))\n",
    "env2 = Input(shape=(time_window, 1))\n",
    "\n",
    "conv1 = Conv1D(filters=16, kernel_size=16, activation='relu', padding='causal')(eeg)\n",
    "\n",
    "cos_sim1 = tf.keras.layers.Dot(axes=(1, 1), normalize=True)([conv1, env1])\n",
    "cos_sim2 = tf.keras.layers.Dot(axes=(1, 1), normalize=True)([conv1, env2])\n",
    "\n",
    "# Classification\n",
    "out1 = Dense(1, activation=\"sigmoid\")(tf.keras.layers.Flatten()(tf.keras.layers.Concatenate()([cos_sim1, cos_sim2])))\n",
    "\n",
    "# 1 output per batch\n",
    "out = tf.keras.layers.Reshape([1], name=\"output_name\")(out1)\n",
    "model = tf.keras.Model(inputs=[eeg, env1, env2], outputs=[out])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "# Set the number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # TRAINING THE MODEL\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_steps = 0\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for data in data_generator():\n",
    "#         print(len(data))\n",
    "        try:\n",
    "            eeg_train, env1_train, env2_train, eeg_val, env1_val, env2_val = data\n",
    "            # Train the model on the loaded data\n",
    "            if eeg_train is not None:\n",
    "                eeg_train, env1_train, env2_train, labels_train = batch_equalizer(eeg_train, env1_train, env2_train, np.ones(eeg_train.shape[0]))\n",
    "                loss, accuracy = model.train_on_batch([eeg_train, env1_train, env2_train], labels_train)\n",
    "                train_loss += loss\n",
    "                train_acc += accuracy\n",
    "                train_steps += 1\n",
    "\n",
    "                eeg_val, env1_val, env2_val, labels_val = batch_equalizer(eeg_val, env1_val, env2_val, np.ones(eeg_val.shape[0]))\n",
    "                test_metrics = model.evaluate([eeg_val, env1_val, env2_val], labels_val, verbose=0)\n",
    "                val_loss += test_metrics[0]\n",
    "                val_acc += test_metrics[1]\n",
    "        except:\n",
    "            pass\n",
    "    train_loss /= train_steps\n",
    "    train_acc /= train_steps\n",
    "    print(f'Training loss: {train_loss * 100:.2f}%')\n",
    "    print(f'Training accuracy: {train_acc * 100:.2f}%')\n",
    "    val_loss = val_loss / train_steps\n",
    "    val_acc = val_acc / train_steps\n",
    "    print(f\"Validation loss: {val_loss * 100:.2f}\")\n",
    "    print(f\"Validation accuracy: {val_acc * 100:.2f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 600, 64)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 600, 1)       1025        ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 600, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 600, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " dot_6 (Dot)                    (None, 1, 1)         0           ['conv1d_3[0][0]',               \n",
      "                                                                  'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " dot_7 (Dot)                    (None, 1, 1)         0           ['conv1d_3[0][0]',               \n",
      "                                                                  'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 1, 2)         0           ['dot_6[0][0]',                  \n",
      "                                                                  'dot_7[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 2)            0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            3           ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " output_name (Reshape)          (None, 1)            0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,028\n",
      "Trainable params: 1,028\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7060, Test accuracy: 0.5000\n",
      "Test loss: 0.7035, Test accuracy: 0.5380\n",
      "Test loss: 0.6976, Test accuracy: 0.5312\n",
      "Test loss: 0.6476, Test accuracy: 0.6443\n",
      "Test loss: 0.6814, Test accuracy: 0.6023\n",
      "Test loss: 0.6421, Test accuracy: 0.6237\n",
      "Test loss: 0.6384, Test accuracy: 0.6307\n",
      "Test loss: 0.6582, Test accuracy: 0.6250\n",
      "Test loss: 0.6733, Test accuracy: 0.5930\n",
      "Test loss: 0.7478, Test accuracy: 0.5523\n",
      "Test loss: 0.6924, Test accuracy: 0.5833\n",
      "Test loss: 0.6582, Test accuracy: 0.5365\n",
      "Test loss: 0.5990, Test accuracy: 0.6154\n",
      "Test loss: 0.6774, Test accuracy: 0.5952\n",
      "Test loss: 0.6666, Test accuracy: 0.5769\n",
      "Test loss: 0.6291, Test accuracy: 0.6919\n",
      "Test loss: 0.6638, Test accuracy: 0.6212\n",
      "Test loss: 0.7496, Test accuracy: 0.5739\n",
      "Test loss: 0.7090, Test accuracy: 0.5886\n",
      "Test loss: 0.6630, Test accuracy: 0.6111\n",
      "Test loss: 0.7623, Test accuracy: 0.4038\n",
      "Test loss: 0.6524, Test accuracy: 0.6488\n",
      "Test loss: 0.5844, Test accuracy: 0.6905\n",
      "Test loss: 0.6817, Test accuracy: 0.6012\n",
      "Test loss: 0.6710, Test accuracy: 0.5581\n",
      "Test loss: 0.7471, Test accuracy: 0.5056\n",
      "Test loss: 0.6576, Test accuracy: 0.6562\n",
      "Test loss: 0.6271, Test accuracy: 0.6562\n",
      "Test loss: 0.6620, Test accuracy: 0.6023\n",
      "Test loss: 0.7034, Test accuracy: 0.5714\n",
      "Test loss: 0.6865, Test accuracy: 0.5455\n",
      "Test loss: 0.6842, Test accuracy: 0.6000\n",
      "Test loss: 0.6075, Test accuracy: 0.7326\n",
      "Test loss: 0.6655, Test accuracy: 0.5872\n",
      "Test loss: 0.6371, Test accuracy: 0.5909\n",
      "Test loss: 0.6727, Test accuracy: 0.6534\n",
      "Test loss: 0.7714, Test accuracy: 0.5469\n",
      "Test loss: 0.7696, Test accuracy: 0.5000\n",
      "Test loss: 0.6644, Test accuracy: 0.6395\n",
      "Test loss: 0.7315, Test accuracy: 0.5000\n",
      "Test loss: 0.5918, Test accuracy: 0.6000\n",
      "Test loss: 0.5741, Test accuracy: 0.7321\n",
      "Test loss: 0.6568, Test accuracy: 0.5600\n",
      "Test loss: 0.7244, Test accuracy: 0.6800\n",
      "Test loss: 0.8126, Test accuracy: 0.5200\n",
      "Test loss: 0.6567, Test accuracy: 0.5909\n",
      "Test loss: 0.7117, Test accuracy: 0.5000\n",
      "Test loss: 0.4930, Test accuracy: 0.8750\n",
      "Test loss: 0.7096, Test accuracy: 0.5655\n",
      "Test loss: 0.6575, Test accuracy: 0.6061\n",
      "Test loss: 0.6462, Test accuracy: 0.6875\n",
      "Test loss: 0.7046, Test accuracy: 0.5570\n",
      "Test loss: 0.6124, Test accuracy: 0.6705\n",
      "Test loss: 0.5936, Test accuracy: 0.7500\n",
      "Test loss: 0.6652, Test accuracy: 0.6042\n",
      "Test loss: 0.6574, Test accuracy: 0.6395\n",
      "Test loss: 0.7622, Test accuracy: 0.4702\n",
      "Test loss: 0.6392, Test accuracy: 0.6094\n",
      "Test loss: 0.6966, Test accuracy: 0.5611\n",
      "Test loss: 0.6958, Test accuracy: 0.5800\n",
      "Test loss: 0.6807, Test accuracy: 0.5655\n",
      "Test loss: 0.6671, Test accuracy: 0.6548\n",
      "Test loss: 0.5829, Test accuracy: 0.6535\n",
      "Epoch end\n",
      "Test loss: 0.6720, Test accuracy: 0.6009\n"
     ]
    }
   ],
   "source": [
    "# TESTING THE MODEL\n",
    "\n",
    "# Initialize variables for accumulating test results\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "num_test_batches = 0\n",
    "\n",
    "# Iterate through the test generator\n",
    "for eeg, env1, env2 in val_data_generator():\n",
    "    # Test the model on the current batch\n",
    "    if eeg is not None:\n",
    "        eeg, env1, env2, labels = batch_equalizer(eeg, env1, env2, np.ones(eeg.shape[0]))\n",
    "        test_metrics = model.test_on_batch([eeg, env1, env2], labels)\n",
    "        test_loss += test_metrics[0]\n",
    "        test_accuracy += test_metrics[1]\n",
    "        num_test_batches += 1\n",
    "        print(\"Test loss: {:.4f}, Test accuracy: {:.4f}\".format(test_metrics[0], test_metrics[1]))\n",
    "\n",
    "# Calculate the average test loss and accuracy\n",
    "avg_test_loss = test_loss / num_test_batches\n",
    "avg_test_accuracy = test_accuracy / num_test_batches\n",
    "\n",
    "print(\"Test loss: {:.4f}, Test accuracy: {:.4f}\".format(avg_test_loss, avg_test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRUN1nnem52N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "* The EEG data preprocessing has been explained in another tutorial.\n",
    "* we have already implemeted method a) linear decoder baseline\n",
    "\n",
    "**Convolutional baseline network**\n",
    "* The first step in the model is a convolutional layer, A (64 x 16) spatio-temporal filter is shifted over the input matrix, containing the EEG.\n",
    "* A rectifying linear unit (ReLu) activation function is used after the convolution step. the kernel size of 16 is chosen because, as is the case in the linear model, we want to look to future EEG to predict the current envelope. the EEG is sampled at fs=64Hz, giving us a temporal resolution of 16/64 = 250ms.\n",
    "* The output of the convolutional block is a (time-window, 1) signal. \n",
    "* In the next step, we calculate the cosine similarity between this signal and both of the envelopes. We will calculate this cosine similarity by applying a *dot product* between the signal and both envelopes. \n",
    "* As a last step, we then have to choose which one of the two attended envelopes is the one we want to choose. We do this by applying a single neuron ( **dense layer** in keras, with a sigmoid activation function. \n",
    "\n",
    "**deep learning model**\n",
    "* the idea here is the same. We still give EEG and envelopes to the model, there are just more processing steps in between before we have to make a decision. \n",
    "* we first apply a one-dimensional convolution to the EEG, with 8 output filters. We can interpret this as kind of a non-linear dimensionality reduction, as the resulting EEG has shape (time-window, 8) instead of the original (time-window, 64) \n",
    "* next, there are some convolutional blocks. These convolutions are applied to both EEG and envelopes. We will have separate track for the EEG ( eg 1 convolutional block) and one for the envelope (eg. also 1 convolutional block). To keep the model stable and simple, we will have one 'track' for the envelopes. Both attended and unattended envelope will be transformed by the same convolutional block, ensuring that the model has to learn to distinguish between the attended and unattended envelope.\n",
    "* after that, we once again compute the dot product and subsequently put the result of this in a sigmoid neuron to reach an end decision.\n",
    "* the possibilities are endless, and we can try to add more convolutional blocks, or even add a recurrent layer ( LSTM blocks) to the model. What is important is that you start from a simple model, and then gradually expand it. this way, if something does not work, it is easier to find the problem, or to revert back to a simpler model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hmbrPpkm52O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LEN 46200\n",
      "=================== DATA LOADED ===================\n",
      "EEG SHAPE: (46200, 64)\n",
      "ATTENDED SHAPE: (46200,)\n",
      "UNATTENDED SHAPE: (46200,)\n",
      "===================================================\n",
      "SPLITTING THE DATA INTO TRAIN AND TEST\n",
      "==================== DATA SHAPES =====================\n",
      "X_train_eeg:(61, 600, 64)\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 28ms/step - loss: 4.6652 - accuracy: 0.4918 - val_loss: 2.4658 - val_accuracy: 0.8125\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 3.1340 - accuracy: 0.7377 - val_loss: 2.9439 - val_accuracy: 0.7500\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 3.2906 - accuracy: 0.7541 - val_loss: 3.7742 - val_accuracy: 0.6875\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 2.5733 - accuracy: 0.8197 - val_loss: 4.3398 - val_accuracy: 0.6875\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 2.3824 - accuracy: 0.8361 - val_loss: 4.6283 - val_accuracy: 0.6250\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 2.2814 - accuracy: 0.8525 - val_loss: 4.9117 - val_accuracy: 0.6250\n",
      "Validation loss: 4.911746501922607\n",
      "Validation accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "# START WITH A SPECIFIC EEG FILE\n",
    "filename = \"sub-001_-_audiobook_1\"\n",
    "filepath = \"/data/eeg/\" + filename + \".npz\"\n",
    "eeg_data = np.load(os.getcwd() + filepath)\n",
    "\n",
    "eeg_preprocessed = np.load(os.getcwd() + \"/data/train_cnn/\" + filename + \".npy\")\n",
    "attended_preprocessed = np.load(os.getcwd() + \"/data/train_cnn/\" + \\\n",
    "                           str(eeg_data[\"stimulus_attended\"]).replace(\".wav\", \"\") + \".npy\")\n",
    "unattended_preprocessed = np.load(os.getcwd() + \"/data/train_cnn/\" + \\\n",
    "                           str(eeg_data[\"stimulus_unattended\"]).replace(\".wav\", \"\") + \".npy\")\n",
    "time_window = 60 * 10\n",
    "\n",
    "data_length = (eeg_preprocessed.shape[0] // time_window) * time_window\n",
    "print(f\"DATA LEN {data_length}\")\n",
    "eeg_preprocessed = eeg_preprocessed[:data_length]\n",
    "\n",
    "attended_preprocessed = attended_preprocessed[:eeg_preprocessed.shape[0]]\n",
    "unattended_preprocessed = unattended_preprocessed[:eeg_preprocessed.shape[0]]\n",
    "print(\"=================== DATA LOADED ===================\")\n",
    "print(f\"EEG SHAPE: {eeg_preprocessed.shape}\")\n",
    "print(f\"ATTENDED SHAPE: {attended_preprocessed.shape}\")\n",
    "print(f\"UNATTENDED SHAPE: {unattended_preprocessed.shape}\")\n",
    "print(\"===================================================\")\n",
    "\n",
    "\n",
    "# Reshape the attended and unattended envelopes to have an extra dimension\n",
    "attended_preprocessed = attended_preprocessed.reshape(-1, time_window, 1)\n",
    "unattended_preprocessed = unattended_preprocessed.reshape(-1, time_window, 1)\n",
    "\n",
    "# Reshape the EEG data\n",
    "eeg_preprocessed = eeg_preprocessed.reshape(-1, time_window, 64)\n",
    "\n",
    "# Create labels for the attended (1) and unattended (0) envelopes\n",
    "labels = np.ones(eeg_preprocessed.shape[0])\n",
    "\n",
    "print(\"SPLITTING THE DATA INTO TRAIN AND TEST\")\n",
    "# Split the data into training and validation sets\n",
    "X_train_eeg, X_val_eeg, y_train, y_val = train_test_split(eeg_preprocessed, labels, test_size=0.2, random_state=42)\n",
    "X_train_env1, X_val_env1, _, _ = train_test_split(attended_preprocessed, labels, test_size=0.2, random_state=42)\n",
    "X_train_env2, X_val_env2, _, _ = train_test_split(unattended_preprocessed, labels, test_size=0.2, random_state=42)\n",
    "print(\"==================== DATA SHAPES =====================\")\n",
    "print(f\"X_train_eeg:{X_train_eeg.shape}\")\n",
    "\n",
    "# cos_sim_diff = tf.keras.layers.Subtract()([cos_sim1, cos_sim2])\n",
    "# gap = GlobalAveragePooling1D()(cos_sim_diff)\n",
    "# output_layer = Dense(1, activation='sigmoid')(gap)\n",
    "# flattened_output = Flatten()(output_layer)\n",
    "\n",
    "# model = Model(inputs=[eeg, env1, env2], outputs=flattened_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Define the model architecture\n",
    "eeg = Input(shape=(time_window, 64))\n",
    "env1 = Input(shape=(time_window, 1))\n",
    "env2 = Input(shape=(time_window, 1))\n",
    "\n",
    "conv1 = Conv1D(filters=64, kernel_size=16, activation='relu', padding='same')(eeg)\n",
    "\n",
    "cos_sim1 = tf.keras.layers.Dot(axes=(1, 1))([conv1, env1])\n",
    "cos_sim2 = tf.keras.layers.Dot(axes=(1, 1))([conv1, env2])\n",
    "\n",
    "# Classification\n",
    "out1 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(\n",
    "    tf.keras.layers.Flatten()(tf.keras.layers.Concatenate()([cos_sim1, cos_sim2])))\n",
    "\n",
    "# 1 output per batch\n",
    "out = tf.keras.layers.Reshape([1], name=\"output_name\")(out1)\n",
    "model = tf.keras.Model(inputs=[eeg, env1, env2], outputs=[out])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_train_eeg, X_train_env1, X_train_env2], y_train, \\\n",
    "                    epochs=50, batch_size=32, \\\n",
    "                    validation_data=([X_val_eeg, X_val_env1, X_val_env2], y_val), \\\n",
    "                    verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate([X_val_eeg, X_val_env1, X_val_env2], y_val, verbose=0)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Validation accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvReB1npm52U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== EEG DATA ======================\n",
      "Attended stimulus: audiobook_1.wav\n",
      "Unattended stimulus: audiobook_5_3.wav\n",
      "Sampling rate: 128 Hz\n",
      "Subject: sub-002\n",
      "======================================================\n",
      "\n",
      "47400\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "[[1.1923741e-07]\n",
      " [7.4550163e-32]\n",
      " [7.0912868e-01]\n",
      " [0.0000000e+00]\n",
      " [1.4917280e-14]\n",
      " [1.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.0328616e-23]\n",
      " [9.9997938e-01]\n",
      " [1.5967587e-11]\n",
      " [2.6795350e-19]\n",
      " [7.2806943e-03]\n",
      " [2.3641388e-35]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [3.5163304e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [4.5474648e-01]\n",
      " [1.0000000e+00]\n",
      " [9.6886784e-01]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [5.5987819e-12]\n",
      " [7.3144744e-18]\n",
      " [3.1110334e-14]\n",
      " [9.0284890e-32]\n",
      " [3.1508523e-18]\n",
      " [0.0000000e+00]\n",
      " [1.4467314e-05]\n",
      " [2.4977141e-06]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [8.4561847e-02]\n",
      " [1.0000000e+00]\n",
      " [5.6154319e-20]\n",
      " [2.0837592e-21]\n",
      " [1.0000000e+00]\n",
      " [1.1374125e-18]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [5.9712793e-06]\n",
      " [1.8558420e-32]\n",
      " [0.0000000e+00]\n",
      " [1.7568204e-18]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [7.6455941e-16]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [4.1493926e-23]\n",
      " [6.8709758e-17]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [2.8671771e-02]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [9.9984652e-01]\n",
      " [4.6311540e-04]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]\n",
      " [1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Load new EEG data\n",
    "new_filename = \"sub-002_-_audiobook_1\"  # Replace this with the name of the new file\n",
    "new_filepath = \"/data/eeg/\" + new_filename + \".npz\"\n",
    "new_eeg_data = np.load(os.getcwd() + new_filepath)\n",
    "\n",
    "print(\"====================== EEG DATA ======================\")\n",
    "print(f\"Attended stimulus: {new_eeg_data['stimulus_attended']}\")\n",
    "print(f\"Unattended stimulus: {new_eeg_data['stimulus_unattended']}\")\n",
    "print(f\"Sampling rate: {new_eeg_data['fs']} Hz\")\n",
    "print(f\"Subject: {new_eeg_data['subject']}\")\n",
    "print(\"======================================================\\n\")\n",
    "new_eeg_preprocessed = np.load(os.getcwd() + \"/data/train_cnn/\" + new_filename + \".npy\")\n",
    "new_attended_preprocessed = np.load(os.getcwd() + \"/data/train_cnn/\" + \\\n",
    "                               str(new_eeg_data[\"stimulus_attended\"]).replace(\".wav\", \"\") + \".npy\")\n",
    "new_unattended_preprocessed = np.load(os.getcwd() + \"/data/train_cnn/\" + \\\n",
    "                               str(new_eeg_data[\"stimulus_unattended\"]).replace(\".wav\", \"\") + \".npy\")\n",
    "\n",
    "# Preprocess the new data like the training data\n",
    "min_data_len = min(new_eeg_preprocessed.shape[0], new_attended_preprocessed.shape[0], new_unattended_preprocessed.shape[0])\n",
    "new_data_length = (min_data_len // time_window) * time_window\n",
    "print(new_data_length)\n",
    "new_eeg_preprocessed = new_eeg_preprocessed[:new_data_length]\n",
    "new_attended_preprocessed = new_attended_preprocessed[:new_data_length]\n",
    "new_unattended_preprocessed = new_unattended_preprocessed[:new_data_length]\n",
    "\n",
    "new_eeg_preprocessed = new_eeg_preprocessed.reshape(-1, time_window, 64)\n",
    "new_attended_preprocessed = new_attended_preprocessed.reshape(-1, time_window, 1)\n",
    "new_unattended_preprocessed = new_unattended_preprocessed.reshape(-1, time_window, 1)\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = model.predict([new_eeg_preprocessed, new_unattended_preprocessed, new_attended_preprocessed])\n",
    "# Apply the threshold (0.5) to convert probabilities to binary class labels\n",
    "binary_predictions = (predictions >= 0.5).astype(int)\n",
    "print(predictions)\n",
    "# The predictions variable will contain the model's predictions for the new data.\n",
    "# You can then analyze these predictions as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we start training the model, we need to make sure that the data is equally balanced. We have attended and unattended envelopes that we give to the model. If we always put the attended envelope at stream 1 and the unattended at stream 2, the model will quickly figure out that it should just always output stream 1 and hence not learn anything. \n",
    "\n",
    "The solution to this is to present each segment of EEG twice, where we swap the envelopes, ( and thus, the labels), from place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def batch_equalizer(eeg, env_1, env_2, labels):\n",
    "    # present each of the eeg segments twice, where the envelopes, and thus the labels \n",
    "    # are swapped around. EEG presented in small segments [bs, window_length, 64]\n",
    "    return (np.concatenate([eeg, eeg], axis=0), np.concatenate([env_1, env_2], axis=0),np.concatenate([ env_2, env_1], axis=0)), np.concatenate([labels, (labels+1)%2], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3mzuy-jm52i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data loading\n",
    "\n",
    "* as you can see, the total amount of data is quite a few GB. this will most probably not fit in your RAM, so we will have to load the data in batches.\n",
    "* Python generators are a great way to do this.\n",
    "* Now we prepare our data to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_generator \u001b[38;5;241m=\u001b[39m \u001b[43mDataGenerator\u001b[49m(files)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# create tf dataset from generator\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[1;32m      5\u001b[0m         data_generator)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataGenerator' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-008_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-004_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-013_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-019_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_3.wav\n",
      "sub-018_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-035_-_podcast_6.npz: ATTENDED podcast_6.wav UNATTENDED podcast_5.wav\n",
      "sub-035_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_6.wav\n",
      "sub-002_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-019_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_4.wav\n",
      "sub-011_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-017_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-019_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-012_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-001_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-029_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_3.wav\n",
      "sub-035_-_podcast_7.npz: ATTENDED podcast_7.wav UNATTENDED audiobook_1.wav\n",
      "sub-009_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_15_13.wav\n",
      "sub-018_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_1.wav\n",
      "sub-016_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-010_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-001_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-003_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-018_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-008_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-015_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-013_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-018_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-003_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_3.wav\n",
      "sub-035_-_podcast_5.npz: ATTENDED podcast_5.wav UNATTENDED audiobook_8_1.wav\n",
      "sub-002_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_4.wav\n",
      "sub-019_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-030_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_1.wav\n",
      "sub-011_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-002_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-012_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-014_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-001_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-009_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_15_38.wav\n",
      "sub-018_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-010_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_1.wav\n",
      "sub-003_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-004_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-005_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-007_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-012_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-014_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-018_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_3.wav\n",
      "sub-039_-_podcast_12.npz: ATTENDED podcast_12.wav UNATTENDED podcast_10.wav\n",
      "sub-034_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_8.wav\n",
      "sub-037_-_audiobook_9_1.npz: ATTENDED audiobook_9_1.wav UNATTENDED podcast_12.wav\n",
      "sub-005_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-016_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-005_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_3.wav\n",
      "sub-015_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-013_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-006_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-028_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_7_5.wav\n",
      "sub-032_-_podcast_5.npz: ATTENDED podcast_5.wav UNATTENDED audiobook_8_1.wav\n",
      "sub-037_-_podcast_9.npz: ATTENDED podcast_9.wav UNATTENDED podcast_11.wav\n",
      "sub-019_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-007_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-009_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_15_12.wav\n",
      "sub-014_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-019_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-032_-_podcast_7.npz: ATTENDED podcast_7.wav UNATTENDED podcast_6.wav\n",
      "sub-006_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-039_-_podcast_11.npz: ATTENDED podcast_11.wav UNATTENDED podcast_12.wav\n",
      "sub-038_-_podcast_9.npz: ATTENDED podcast_9.wav UNATTENDED audiobook_9_1.wav\n",
      "sub-005_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-031_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_7_3.wav\n",
      "sub-016_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-010_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-015_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-039_-_podcast_10.npz: ATTENDED podcast_10.wav UNATTENDED audiobook_1.wav\n",
      "sub-032_-_podcast_6.npz: ATTENDED podcast_6.wav UNATTENDED podcast_8.wav\n",
      "sub-008_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-011_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-005_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-007_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-009_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_15_27.wav\n",
      "sub-006_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-012_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-037_-_podcast_12.npz: ATTENDED podcast_12.wav UNATTENDED podcast_10.wav\n",
      "sub-037_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_9.wav\n",
      "sub-013_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-039_-_audiobook_9_1.npz: ATTENDED audiobook_9_1.wav UNATTENDED podcast_11.wav\n",
      "sub-005_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-036_-_podcast_8.npz: ATTENDED podcast_8.wav UNATTENDED audiobook_1.wav\n",
      "sub-003_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-010_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-018_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-013_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-008_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-011_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-003_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-019_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-002_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-004_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-012_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_3.wav\n",
      "sub-038_-_audiobook_9_1.npz: ATTENDED audiobook_9_1.wav UNATTENDED podcast_12.wav\n",
      "sub-007_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-033_-_podcast_5.npz: ATTENDED podcast_5.wav UNATTENDED audiobook_8_1.wav\n",
      "sub-009_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_15_17.wav\n",
      "sub-033_-_podcast_7.npz: ATTENDED podcast_7.wav UNATTENDED podcast_5.wav\n",
      "sub-007_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-012_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-014_-_audiobook_1_1.npz: ATTENDED audiobook_1_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-039_-_podcast_9.npz: ATTENDED podcast_9.wav UNATTENDED audiobook_9_1.wav\n",
      "sub-022_-_audiobook_1_2_shifted.npz: ATTENDED audiobook_1_2_shifted.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-011_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-003_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-037_-_podcast_11.npz: ATTENDED podcast_11.wav UNATTENDED audiobook_9_1.wav\n",
      "sub-008_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-032_-_audiobook_8_1.npz: ATTENDED audiobook_8_1.wav UNATTENDED podcast_7.wav\n",
      "sub-003_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-032_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_5.wav\n",
      "sub-010_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-018_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-010_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-013_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-008_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-037_-_podcast_10.npz: ATTENDED podcast_10.wav UNATTENDED audiobook_1.wav\n",
      "sub-006_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-002_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-011_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-019_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-002_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-033_-_audiobook_8_1.npz: ATTENDED audiobook_8_1.wav UNATTENDED podcast_6.wav\n",
      "sub-006_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-033_-_podcast_6.npz: ATTENDED podcast_6.wav UNATTENDED podcast_8.wav\n",
      "sub-009_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_15_39.wav\n",
      "sub-006_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-034_-_podcast_6.npz: ATTENDED podcast_6.wav UNATTENDED podcast_5.wav\n",
      "sub-015_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-007_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-016_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-036_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_6.wav\n",
      "sub-035_-_audiobook_8_1.npz: ATTENDED audiobook_8_1.wav UNATTENDED podcast_8.wav\n",
      "sub-017_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-019_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-014_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-038_-_podcast_12.npz: ATTENDED podcast_12.wav UNATTENDED podcast_10.wav\n",
      "sub-007_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-016_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-002_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-018_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-005_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-006_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-034_-_audiobook_8_1.npz: ATTENDED audiobook_8_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-034_-_podcast_7.npz: ATTENDED podcast_7.wav UNATTENDED audiobook_8_1.wav\n",
      "sub-034_-_podcast_5.npz: ATTENDED podcast_5.wav UNATTENDED podcast_7.wav\n",
      "sub-006_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-015_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-002_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-038_-_podcast_10.npz: ATTENDED podcast_10.wav UNATTENDED audiobook_1.wav\n",
      "sub-014_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_1_1.wav\n",
      "sub-002_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-033_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_7.wav\n",
      "sub-017_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-036_-_audiobook_8_1.npz: ATTENDED audiobook_8_1.wav UNATTENDED podcast_7.wav\n",
      "sub-014_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-007_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_4.wav\n",
      "sub-038_-_podcast_11.npz: ATTENDED podcast_11.wav UNATTENDED podcast_9.wav\n",
      "sub-016_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-003_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_1.wav\n",
      "sub-005_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-003_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-015_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-007_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-023_-_audiobook_1_1_shifted.npz: ATTENDED audiobook_1_1_shifted.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-023_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1_1_shifted.wav\n",
      "sub-014_-_audiobook_1_2_shifted.npz: ATTENDED audiobook_1_2_shifted.wav UNATTENDED audiobook_4.wav\n",
      "sub-027_-_audiobook_7_2.npz: ATTENDED audiobook_7_2.wav UNATTENDED audiobook_7_5.wav\n",
      "sub-024_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-012_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-026_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-025_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-016_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-025_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-023_-_audiobook_1_2.npz: ATTENDED audiobook_1_2.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-040_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_12.wav\n",
      "sub-027_-_audiobook_7_3.npz: ATTENDED audiobook_7_3.wav UNATTENDED audiobook_7_1.wav\n",
      "sub-024_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-026_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-021_-_audiobook_1_1.npz: ATTENDED audiobook_1_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-034_-_podcast_8.npz: ATTENDED podcast_8.wav UNATTENDED podcast_6.wav\n",
      "sub-022_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_1_1.wav\n",
      "sub-026_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-024_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-027_-_audiobook_7_1.npz: ATTENDED audiobook_7_1.wav UNATTENDED podcast_2.wav\n",
      "sub-022_-_audiobook_1_1.npz: ATTENDED audiobook_1_1.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-026_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-013_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-025_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-012_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-026_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-022_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-020_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-021_-_audiobook_1_2_shifted.npz: ATTENDED audiobook_1_2_shifted.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-027_-_audiobook_7_4.npz: ATTENDED audiobook_7_4.wav UNATTENDED audiobook_7_2.wav\n",
      "sub-023_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-021_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-013_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-036_-_podcast_7.npz: ATTENDED podcast_7.wav UNATTENDED podcast_8.wav\n",
      "sub-031_-_podcast_3.npz: ATTENDED podcast_3.wav UNATTENDED podcast_2.wav\n",
      "sub-028_-_podcast_2.npz: ATTENDED podcast_2.wav UNATTENDED audiobook_7_2.wav\n",
      "sub-028_-_podcast_3.npz: ATTENDED podcast_3.wav UNATTENDED audiobook_7_6.wav\n",
      "sub-016_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-022_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-031_-_podcast_2.npz: ATTENDED podcast_2.wav UNATTENDED podcast_1.wav\n",
      "sub-036_-_podcast_6.npz: ATTENDED podcast_6.wav UNATTENDED podcast_5.wav\n",
      "sub-021_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_1_2_shifted.wav\n",
      "sub-027_-_podcast_1.npz: ATTENDED podcast_1.wav UNATTENDED audiobook_7_3.wav\n",
      "sub-027_-_audiobook_7_5.npz: ATTENDED audiobook_7_5.wav UNATTENDED podcast_1.wav\n",
      "sub-027_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_3.wav\n",
      "sub-020_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-023_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-040_-_audiobook_9_1.npz: ATTENDED audiobook_9_1.wav UNATTENDED podcast_9.wav\n",
      "sub-033_-_podcast_8.npz: ATTENDED podcast_8.wav UNATTENDED audiobook_1.wav\n",
      "sub-027_-_podcast_3.npz: ATTENDED podcast_3.wav UNATTENDED audiobook_7_4.wav\n",
      "sub-023_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_1_2.wav\n",
      "sub-021_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-012_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-016_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-022_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-028_-_podcast_1.npz: ATTENDED podcast_1.wav UNATTENDED audiobook_7_1.wav\n",
      "sub-022_-_audiobook_5_2.npz: ATTENDED audiobook_5_2.wav UNATTENDED audiobook_4.wav\n",
      "sub-031_-_podcast_1.npz: ATTENDED podcast_1.wav UNATTENDED audiobook_7_1.wav\n",
      "sub-036_-_podcast_5.npz: ATTENDED podcast_5.wav UNATTENDED audiobook_8_1.wav\n",
      "sub-013_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1.wav\n",
      "sub-023_-_audiobook_5_3.npz: ATTENDED audiobook_5_3.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-027_-_podcast_2.npz: ATTENDED podcast_2.wav UNATTENDED audiobook_1.wav\n",
      "sub-026_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-020_-_audiobook_5_1.npz: ATTENDED audiobook_5_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-031_-_audiobook_7_5.npz: ATTENDED audiobook_7_5.wav UNATTENDED audiobook_7_4.wav\n",
      "sub-021_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-030_-_podcast_3.npz: ATTENDED podcast_3.wav UNATTENDED audiobook_7_4.wav\n",
      "sub-038_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_11.wav\n",
      "sub-029_-_podcast_2.npz: ATTENDED podcast_2.wav UNATTENDED audiobook_7_2.wav\n",
      "sub-029_-_audiobook_7_4.npz: ATTENDED audiobook_7_4.wav UNATTENDED audiobook_1.wav\n",
      "sub-025_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-030_-_audiobook_7_4.npz: ATTENDED audiobook_7_4.wav UNATTENDED audiobook_7_1.wav\n",
      "sub-010_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-021_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-014_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-028_-_audiobook_7_5.npz: ATTENDED audiobook_7_5.wav UNATTENDED audiobook_7_3.wav\n",
      "sub-009_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_15_9.wav\n",
      "sub-028_-_audiobook_7_4.npz: ATTENDED audiobook_7_4.wav UNATTENDED podcast_1.wav\n",
      "sub-024_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-008_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_1.wav\n",
      "sub-015_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-030_-_audiobook_7_5.npz: ATTENDED audiobook_7_5.wav UNATTENDED podcast_3.wav\n",
      "sub-024_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-029_-_podcast_3.npz: ATTENDED podcast_3.wav UNATTENDED audiobook_7_1.wav\n",
      "sub-029_-_audiobook_7_5.npz: ATTENDED audiobook_7_5.wav UNATTENDED audiobook_7_4.wav\n",
      "sub-030_-_podcast_2.npz: ATTENDED podcast_2.wav UNATTENDED audiobook_7_2.wav\n",
      "sub-020_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-026_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-031_-_audiobook_7_4.npz: ATTENDED audiobook_7_4.wav UNATTENDED audiobook_7_2.wav\n",
      "sub-031_-_audiobook_7_6.npz: ATTENDED audiobook_7_6.wav UNATTENDED audiobook_1.wav\n",
      "sub-023_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-025_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-029_-_podcast_1.npz: ATTENDED podcast_1.wav UNATTENDED audiobook_7_3.wav\n",
      "sub-024_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_4.wav\n",
      "sub-032_-_podcast_8.npz: ATTENDED podcast_8.wav UNATTENDED audiobook_1.wav\n",
      "sub-011_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-015_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-028_-_audiobook_7_6.npz: ATTENDED audiobook_7_6.wav UNATTENDED audiobook_7_4.wav\n",
      "sub-009_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-024_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-022_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_1_2_shifted.wav\n",
      "sub-010_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-030_-_audiobook_7_6.npz: ATTENDED audiobook_7_6.wav UNATTENDED audiobook_7_5.wav\n",
      "sub-029_-_audiobook_7_6.npz: ATTENDED audiobook_7_6.wav UNATTENDED audiobook_7_5.wav\n",
      "sub-030_-_podcast_1.npz: ATTENDED podcast_1.wav UNATTENDED podcast_2.wav\n",
      "sub-026_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-031_-_audiobook_7_3.npz: ATTENDED audiobook_7_3.wav UNATTENDED audiobook_7_5.wav\n",
      "sub-040_-_podcast_10.npz: ATTENDED podcast_10.wav UNATTENDED audiobook_9_1.wav\n",
      "sub-020_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-039_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED podcast_9.wav\n",
      "sub-024_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-022_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_3.wav\n",
      "sub-029_-_audiobook_7_2.npz: ATTENDED audiobook_7_2.wav UNATTENDED podcast_1.wav\n",
      "sub-011_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-030_-_audiobook_7_2.npz: ATTENDED audiobook_7_2.wav UNATTENDED audiobook_7_3.wav\n",
      "sub-020_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_3.wav\n",
      "sub-015_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-028_-_audiobook_7_3.npz: ATTENDED audiobook_7_3.wav UNATTENDED podcast_2.wav\n",
      "sub-008_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-023_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-025_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-009_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_15_33.wav\n",
      "sub-028_-_audiobook_7_2.npz: ATTENDED audiobook_7_2.wav UNATTENDED podcast_3.wav\n",
      "sub-014_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_1_2_shifted.wav\n",
      "sub-030_-_audiobook_7_3.npz: ATTENDED audiobook_7_3.wav UNATTENDED audiobook_7_6.wav\n",
      "sub-035_-_podcast_8.npz: ATTENDED podcast_8.wav UNATTENDED podcast_7.wav\n",
      "sub-029_-_audiobook_7_3.npz: ATTENDED audiobook_7_3.wav UNATTENDED audiobook_7_6.wav\n",
      "sub-025_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-021_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_5_3.wav\n",
      "sub-021_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_1_1.wav\n",
      "sub-040_-_podcast_11.npz: ATTENDED podcast_11.wav UNATTENDED podcast_10.wav\n",
      "sub-031_-_audiobook_7_2.npz: ATTENDED audiobook_7_2.wav UNATTENDED podcast_3.wav\n",
      "sub-022_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-025_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-029_-_audiobook_7_1.npz: ATTENDED audiobook_7_1.wav UNATTENDED podcast_2.wav\n",
      "sub-010_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_6_2.wav\n",
      "sub-030_-_audiobook_7_1.npz: ATTENDED audiobook_7_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-020_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_5_2.wav\n",
      "sub-026_-_audiobook_2_1.npz: ATTENDED audiobook_2_1.wav UNATTENDED audiobook_4.wav\n",
      "sub-023_-_audiobook_6_2.npz: ATTENDED audiobook_6_2.wav UNATTENDED audiobook_5_1.wav\n",
      "sub-008_-_audiobook_1.npz: ATTENDED audiobook_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-028_-_audiobook_7_1.npz: ATTENDED audiobook_7_1.wav UNATTENDED audiobook_1.wav\n",
      "sub-040_-_podcast_9.npz: ATTENDED podcast_9.wav UNATTENDED audiobook_1.wav\n",
      "sub-011_-_audiobook_3.npz: ATTENDED audiobook_3.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-024_-_audiobook_4.npz: ATTENDED audiobook_4.wav UNATTENDED audiobook_6_1.wav\n",
      "sub-021_-_audiobook_6_1.npz: ATTENDED audiobook_6_1.wav UNATTENDED audiobook_2_1.wav\n",
      "sub-040_-_podcast_12.npz: ATTENDED podcast_12.wav UNATTENDED podcast_11.wav\n",
      "sub-031_-_audiobook_7_1.npz: ATTENDED audiobook_7_1.wav UNATTENDED audiobook_7_6.wav\n"
     ]
    }
   ],
   "source": [
    "data_folder = os.getcwd() + '/data/eeg'\n",
    "all_files = [f for f in os.listdir(data_folder) if f.endswith('.npz')]\n",
    "\n",
    "for file in all_files:\n",
    "    data = np.load(data_folder + '/' + file)\n",
    "    attended = data[\"stimulus_attended\"]\n",
    "    unattended = data[\"stimulus_unattended\"]\n",
    "    print(f\"{file}: ATTENDED {attended} UNATTENDED {unattended}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_list = os.listdir(os.getcwd() + \"/data/eeg/\")\n",
    "# print(dir_list)\n",
    "# data_generator = DataGenerator(files)\n",
    "\n",
    "# # create tf dataset from generator\n",
    "# dataset = tf.data.Dataset.from_generator(\n",
    "#         data_generator)\n",
    "\n",
    "# now you have a dataset, you can perform operations on the fly (  using built-in functions such as 'map', 'window', 'batch', etc)\n",
    "# eg. window the dataset into slices of (EEG, envelope1, envelope2) of a certain length, with a hop size between consecutive slices\n",
    "# batch the data into batches of a certain size\n",
    "# shuffle the data\n",
    "# create a corect label for each sample ( is envelope 1 attended or envelope2 , eg, label 1 or 0 )\n",
    "\n",
    "# using keras, we can easily create a model that can be trained on this dataset, giving this dataset to the model.fit() function\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_S_D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
